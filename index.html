<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>STAT 25100 Final Exam Master Summary</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <h1>STAT 25100 Final Exam Study Guide </h1>

  <nav class="navbar">
  <ul>
    <li><a href="#l10-11">PDF & CDF</a></li>
    <li><a href="#l12">Expected Value</a></li>
    <li><a href="#l13">Joint Distributions</a></li>
    <li><a href="#l14">Conditional</a></li>
    <li><a href="#l15">Sums & Orders</a></li>
    <li><a href="#l16">Covariance</a></li>
    <li><a href="#l18">Conditional Exp</a></li>
    <li><a href="#l19">MGFs</a></li>
    <li><a href="#lCLT">CLT & Bounds</a></li>
  </ul>
</nav>

  <!-- All concepts and equations are already combined into this single version -->

  <!-- CONTENT WAS PREVIOUSLY MERGED: this version is already complete with MathJax and concepts -->

  <div class="concept" id="l10-11">
    <h2>L10–L11: Continuous Random Variables & Transformations</h2>
    <ul>
      <li><strong>PDF Validity:</strong> Must be nonnegative and \( \int f(x)\,dx = 1 \).</li>
      <li><strong>Properties of PDFs:</strong> May be >1, not always continuous, not a probability directly.</li>
      <li><strong>CDF:</strong> Non-decreasing, between 0 and 1. Continuous for continuous r.v., stepwise for discrete.</li>
      <li><strong>Find PDF from CDF:</strong> \( f(x) = \frac{d}{dx}F(x) \)</li>
      <li><strong>Find Probability from CDF:</strong> \( P(a < X \leq b) = F(b) - F(a) \)</li>
      <li><strong>Transformation:</strong> \( Y = g(X) \Rightarrow f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy}g^{-1}(y) \right| \)</li>
      <li><strong>Linear Transform:</strong> \( Y = aX + b \Rightarrow f_Y(y) = \frac{1}{|a|} f_X\left(\frac{y - b}{a}\right) \)</li>
    </ul>
  </div>

  <div class="concept" id="l12">
    <h2>L12: Expected Values, Variance & Hazard Rate</h2>
    <ul>
      <li><strong>Expected Value:</strong> \( E[X] = \int x f(x) \, dx \)</li>
      <li><strong>Variance:</strong> \( \mathrm{Var}(X) = E[X^2] - (E[X])^2 \)</li>
      <li><strong>Function Expectation:</strong> \( E[g(X)] = \int g(x) f(x) \, dx \)</li>
      <li><strong>Linear Effects:</strong> \( E[aX + b] = aE[X] + b \), \( \mathrm{Var}(aX + b) = a^2 \mathrm{Var}(X) \)</li>
      <li><strong>Hazard Rate:</strong> \( \lambda(x) = \frac{f(x)}{1 - F(x)} \)</li>
      <li><strong>From Hazard to CDF:</strong> \( F(t) = 1 - \exp\left( -\int_0^t \lambda(u) \, du \right) \)</li>
    </ul>
  </div>

  <div class="concept" id="l13">
    <h2>L13: Joint & Marginal Distributions, Independence</h2>
    <ul>
      <li><strong>Joint PDF Validity:</strong> Nonnegative, \( \iint f(x, y) \, dx \, dy = 1 \)</li>
      <li><strong>Marginal PDFs:</strong> \( f_X(x) = \int f(x, y) \, dy \), \( f_Y(y) = \int f(x, y) \, dx \)</li>
      <li><strong>Independence:</strong> \( f(x, y) = f_X(x) f_Y(y) \)</li>
      <li><strong>Poisson Thinning:</strong> \( X \sim \text{Pois}(\lambda p), Y \sim \text{Pois}(\lambda(1-p)) \), independent.</li>
    </ul>
  </div>

  <div class="concept" id="l14">
    <h2>L14: Conditional Distributions</h2>
    <ul>
      <li><strong>Conditional PDF:</strong> \( f_{Y|X}(y|x) = \frac{f(x, y)}{f_X(x)} \)</li>
    </ul>
  </div>

  <div class="concept" id="l15">
    <h2>L15: Sums, Transformations & Order Statistics</h2>
    <ul>
      <li><strong>Sum:</strong> \( T = X + Y \Rightarrow f_T(t) = \int f_X(x) f_Y(t - x) \, dx \)</li>
      <li><strong>Transformations:</strong> Use Jacobian if \( U = g(X,Y), V = h(X,Y) \)</li>
      <li><strong>Order Stats:</strong> \( X_{(1)} = \min(X_i), X_{(n)} = \max(X_i) \)</li>
    </ul>
  </div>

  <div class="concept" id="l16-17">
    <h2>L16–L17: Covariance, Correlation & Properties</h2>
    <ul>
      <li><strong>Covariance:</strong> \( \text{Cov}(X,Y) = E[XY] - E[X]E[Y] \)</li>
      <li><strong>Variance of Sum:</strong> \( \text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab\text{Cov}(X,Y) \)</li>
      <li><strong>Correlation:</strong> \( \rho = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y} \)</li>
    </ul>
  </div>

  <div class="concept" id="l18">
    <h2>L18: Conditional Expectation, Variance, Tower Law</h2>
    <ul>
      <li><strong>Conditional Expectation:</strong> \( E[Y | X] \)</li>
      <li><strong>Tower Law:</strong> \( E[Y] = E[E[Y | X]] \)</li>
      <li><strong>Law of Total Variance:</strong> \( \text{Var}(Y) = \text{Var}(E[Y | X]) + E[\text{Var}(Y | X)] \)</li>
    </ul>
  </div>

  <div class="concept" id="l19">
    <h2>L19: Moment Generating Functions</h2>
    <ul>
      <li><strong>MGF:</strong> \( M_X(t) = E[e^{tX}] \)</li>
      <li><strong>Moments:</strong> \( E[X] = M'(0), E[X^2] = M''(0) \)</li>
      <li><strong>Linear Transform:</strong> \( M_{a + bX}(t) = e^{at} M_X(bt) \)</li>
      <li><strong>Sum of Independent:</strong> \( M_{X+Y}(t) = M_X(t) M_Y(t) \)</li>
    </ul>
  </div>

  <div class="concept" id="lCLT">
    <h2>L12 CLT, Markov, Chebyshev</h2>
    <ul>
      <li><strong>CLT:</strong> \( Z = \frac{\sum X_i - n\mu}{\sqrt{n\sigma^2}} \rightarrow N(0,1) \)</li>
      <li><strong>Markov Inequality:</strong> \( P(X \geq a) \leq \frac{E[X]}{a} \)</li>
      <li><strong>Chebyshev Inequality:</strong> \( P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} \)</li>
    </ul>
  </div>

  <!-- ADDITIONAL TOPICS -->

<div class="concept" id="discrete-rv">
  <h2>Discrete Random Variables</h2>
  <ul>
    <li><strong>PMF:</strong> \( p(x) = P(X = x) \), with \( p(x) \geq 0 \) and \( \sum_x p(x) = 1 \).</li>
    <li><strong>CDF:</strong> \( F(x) = P(X \leq x) = \sum_{t \leq x} p(t) \).</li>
    <li><strong>Expectation:</strong> \( E[X] = \sum_x x \, p(x) \).</li>
    <li><strong>Variance:</strong> \( \mathrm{Var}(X) = \sum_x (x - \mu)^2 p(x) \).</li>
    <li><strong>Examples:</strong>
      <ul>
        <li><em>Bernoulli:</em> \( p(x) = p^x (1-p)^{1-x} \), \( \mu = p \), \( \sigma^2 = p(1-p) \).</li>
        <li><em>Binomial:</em> \( p(x) = \binom{n}{x} p^x (1-p)^{n-x} \).</li>
        <li><em>Geometric:</em> \( p(x) = (1-p)^{x-1} p \), memoryless property.</li>
        <li><em>Poisson:</em> \( p(x) = e^{-\lambda} \frac{\lambda^x}{x!} \), \( \mu = \sigma^2 = \lambda \).</li>
      </ul>
    </li>
  </ul>
</div>

<div class="concept" id="named-cont">
  <h2>Named Continuous Distributions</h2>
  <ul>
    <li><em>Uniform(a,b):</em> \( f(x) = \frac{1}{b-a} \), \( \mu = \frac{a+b}{2} \), \( \sigma^2 = \frac{(b-a)^2}{12} \).</li>
    <li><em>Exponential(λ):</em> \( f(x) = \lambda e^{-\lambda x} \), \( \mu = 1/\lambda \), memoryless.</li>
    <li><em>Normal(μ,σ²):</em> \( f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2 / (2\sigma^2)} \).</li>
    <li><em>Gamma(α,β):</em> \( f(x) = \frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1} e^{-x/\beta} \).</li>
  </ul>
</div>

<div class="concept" id="bayes">
  <h2>Conditional Probability & Bayes Theorem</h2>
  <ul>
    <li><strong>Conditional Probability:</strong> \( P(A|B) = \frac{P(A \cap B)}{P(B)} \).</li>
    <li><strong>Bayes' Theorem:</strong> \( P(A|B) = \frac{P(B|A)P(A)}{P(B)} \).</li>
    <li><strong>Total Probability:</strong> \( P(B) = \sum_i P(B|A_i)P(A_i) \).</li>
  </ul>
</div>

<div class="concept" id="sampling">
  <h2>Sampling Distributions</h2>
  <ul>
    <li><strong>Sample Mean:</strong> \( \bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right) \) if \( X_i \) normal.</li>
    <li><strong>Central Limit Theorem:</strong> For large \( n \), \( \bar{X} \) approx. normal.</li>
    <li><em>t-distribution:</em> Used when σ unknown and data normal.</li>
    <li><em>Chi-square:</em> Sum of squares of standard normals.</li>
    <li><em>F-distribution:</em> Ratio of scaled chi-squares.</li>
  </ul>
</div>

<div class="concept" id="estimation">
  <h2>Point Estimation</h2>
  <ul>
    <li><strong>Bias:</strong> \( \mathrm{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta \).</li>
    <li><strong>Consistency:</strong> \( \hat{\theta} \to_p \theta \) as \( n \to \infty \).</li>
    <li><strong>Efficiency:</strong> Lower variance among unbiased estimators is better.</li>
    <li><strong>MLE:</strong> Maximize \( L(\theta) = \prod f(x_i | \theta) \).</li>
  </ul>
</div>

<div class="concept" id="ci">
  <h2>Confidence Intervals</h2>
  <ul>
    <li><em>Mean, σ known:</em> \( \bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \).</li>
    <li><em>Mean, σ unknown:</em> \( \bar{X} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}} \).</li>
    <li><em>Proportion:</em> \( \hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \).</li>
  </ul>
</div>

<div class="concept" id="hypothesis">
  <h2>Hypothesis Testing</h2>
  <ul>
    <li><strong>Steps:</strong> State \( H_0 \) and \( H_a \), choose α, compute statistic, find p-value, decide.</li>
    <li><strong>Type I Error:</strong> Rejecting \( H_0 \) when true.</li>
    <li><strong>Type II Error:</strong> Failing to reject \( H_0 \) when false.</li>
    <li><st


  <p><em>"Statistics is the grammar of science." </em></p>

  <h2>Interactive PDF and CDF of Normal Distribution</h2>
<div id="pdf-cdf" style="height: 400px;"></div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script>
  const x = Array.from({length: 200}, (_, i) => -4 + i * 0.04);
  const pdf = x.map(val => Math.exp(-0.5 * val * val) / Math.sqrt(2 * Math.PI));
  const cdf = x.map(val => 0.5 * (1 + erf(val / Math.sqrt(2))));

  function erf(x) {
    // approximation of erf(x)
    const sign = x >= 0 ? 1 : -1;
    const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741;
    const a4 = -1.453152027, a5 = 1.061405429;
    const p = 0.3275911;
    const t = 1.0 / (1.0 + p * Math.abs(x));
    const y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);
    return sign * y;
  }

  Plotly.newPlot('pdf-cdf', [
    { x, y: pdf, type: 'scatter', name: 'PDF', line: { color: '#1f77b4' } },
    { x, y: cdf, type: 'scatter', name: 'CDF', line: { color: '#ff7f0e' } }
  ], {
    title: 'Standard Normal Distribution',
    xaxis: { title: 'x' },
    yaxis: { title: 'Probability' },
    legend: { x: 0.1, y: 1.1, orientation: 'h' }
  });
</script>

<h2>Discrete Random Variable Expectation (Fair Die)</h2>
<div id="dice-bar" style="height: 350px;"></div>
<script>
  const x_vals = [1, 2, 3, 4, 5, 6];
  const probs = Array(6).fill(1/6);
  const expectation = x_vals.reduce((acc, x, i) => acc + x * probs[i], 0);

  Plotly.newPlot('dice-bar', [{
    x: x_vals,
    y: probs,
    type: 'bar',
    marker: { color: '#4CAF50' }
  }], {
    title: `PMF of Fair Die (E[X] = ${expectation.toFixed(2)})`,
    xaxis: { title: 'Value of X' },
    yaxis: { title: 'P(X = x)' }
  });
</script>

<h2>Central Limit Theorem Simulation</h2>
<div id="clt" style="height: 400px;"></div>
<input type="range" id="sampleSlider" min="1" max="100" value="1" style="width: 100%;">
<p>Sample size (n): <span id="sliderVal">1</span></p>
<script>
  function getCLTSamples(n, trials = 10000) {
    let results = [];
    for (let i = 0; i < trials; i++) {
      let sample = Array.from({length: n}, () => Math.random());
      let mean = sample.reduce((a, b) => a + b) / n;
      results.push(mean);
    }
    return results;
  }

  function updateCLT(n) {
    const data = getCLTSamples(n);
    Plotly.newPlot('clt', [{
      x: data,
      type: 'histogram',
      marker: { color: '#0077cc' },
      autobinx: true
    }], {
      title: `Distribution of Sample Means (Uniform[0,1], n = ${n})`,
      xaxis: { title: 'Sample Mean' },
      yaxis: { title: 'Frequency' }
    });
    document.getElementById('sliderVal').textContent = n;
  }

  document.getElementById('sampleSlider').addEventListener('input', (e) => {
    updateCLT(Number(e.target.value));
  });

  updateCLT(1);
</script>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</body>
</html>
